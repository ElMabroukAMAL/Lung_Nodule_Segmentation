{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Shb4-H19ZbNT",
        "outputId": "11c3d9e7-faed-42dc-fc3e-2d78303ec330"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# monter Google Drive pour accéder aux données d'entraînement stockées dans un dossier.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx3kGBidGkYB"
      },
      "source": [
        "Prepare paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI20kNp98aLW",
        "outputId": "5cbff0d8-88ce-4e7b-ff56-90373b2d60ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of samples: 9214\n",
            "/content/drive/MyDrive/3P_IIA3/training/trainingNodules/LIDC-IDRI-0001_0_0_5.jpg | /content/drive/MyDrive/3P_IIA3/training/trainingMask/LIDC-IDRI-0001_0_0_5.jpg\n",
            "/content/drive/MyDrive/3P_IIA3/training/trainingNodules/LIDC-IDRI-0001_0_1_5.jpg | /content/drive/MyDrive/3P_IIA3/training/trainingMask/LIDC-IDRI-0001_0_1_5.jpg\n",
            "/content/drive/MyDrive/3P_IIA3/training/trainingNodules/LIDC-IDRI-0001_0_2_5.jpg | /content/drive/MyDrive/3P_IIA3/training/trainingMask/LIDC-IDRI-0001_0_2_5.jpg\n",
            "/content/drive/MyDrive/3P_IIA3/training/trainingNodules/LIDC-IDRI-0001_0_3_5.jpg | /content/drive/MyDrive/3P_IIA3/training/trainingMask/LIDC-IDRI-0001_0_3_5.jpg\n",
            "/content/drive/MyDrive/3P_IIA3/training/trainingNodules/LIDC-IDRI-0001_0_4_5.jpg | /content/drive/MyDrive/3P_IIA3/training/trainingMask/LIDC-IDRI-0001_0_4_5.jpg\n",
            "/content/drive/MyDrive/3P_IIA3/training/trainingNodules/LIDC-IDRI-0001_0_5_5.jpg | /content/drive/MyDrive/3P_IIA3/training/trainingMask/LIDC-IDRI-0001_0_5_5.jpg\n",
            "/content/drive/MyDrive/3P_IIA3/training/trainingNodules/LIDC-IDRI-0001_0_6_5.jpg | /content/drive/MyDrive/3P_IIA3/training/trainingMask/LIDC-IDRI-0001_0_6_5.jpg\n",
            "/content/drive/MyDrive/3P_IIA3/training/trainingNodules/LIDC-IDRI-0001_0_7_5.jpg | /content/drive/MyDrive/3P_IIA3/training/trainingMask/LIDC-IDRI-0001_0_7_5.jpg\n",
            "/content/drive/MyDrive/3P_IIA3/training/trainingNodules/LIDC-IDRI-0001_0_8_5.jpg | /content/drive/MyDrive/3P_IIA3/training/trainingMask/LIDC-IDRI-0001_0_8_5.jpg\n",
            "/content/drive/MyDrive/3P_IIA3/training/trainingNodules/LIDC-IDRI-0002_0_0_5.jpg | /content/drive/MyDrive/3P_IIA3/training/trainingMask/LIDC-IDRI-0002_0_0_5.jpg\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "input_dir = \"/content/drive/MyDrive/3P_IIA3/training/trainingNodules/\"\n",
        "target_dir = \"/content/drive/MyDrive/3P_IIA3/training/trainingMask/\"\n",
        "img_size = (64, 64) #Taille des images redimensionnées en (largeur, hauteur). Toutes les images seront converties en cette dimension pour être compatibles avec le modèle.\n",
        "num_classes = 2     #Nombre de classes dans le modèle (segmentation binaire : présence ou absence de nodule)\n",
        "batch_size = 32     #Nombre d'images par lot.\n",
        "\n",
        "# sorted(...) : Trie les chemins de manière lexicographique pour garantir l'alignement entre les images et les masques\n",
        "input_img_paths = sorted(\n",
        "    [\n",
        "        #Combine le chemin du répertoire (input_dir) avec le nom du fichier (fname) pour obtenir le chemin complet\n",
        "        os.path.join(input_dir, fname)\n",
        "        #Récupère tous les noms de fichiers dans le dossier input_dir\n",
        "        for fname in os.listdir(input_dir)\n",
        "        #Filtre uniquement les fichiers se terminant par .jpg\n",
        "        if fname.endswith(\".jpg\")\n",
        "    ]\n",
        ")\n",
        "target_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(target_dir, fname)\n",
        "        for fname in os.listdir(target_dir)\n",
        "        if fname.endswith(\".jpg\") and not fname.startswith(\".\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "#Affiche le nombre d’images d’entrée disponibles dans input_dir (correspondant normalement au nombre de masques dans target_dir)\n",
        "print(\"Number of samples:\", len(input_img_paths))\n",
        "\n",
        "for input_path, target_path in zip(input_img_paths[:10], target_img_paths[:10]):\n",
        "    print(input_path, \"|\", target_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yEcTxZwoehWH",
        "outputId": "0d283316-4840-4137-a1cf-f47f732afd12",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.3.25)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.6.2)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n"
          ]
        }
      ],
      "source": [
        "pip install --upgrade tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMlYtADOGqYv"
      },
      "source": [
        "What does one input image and corresponding segmentation mask look like?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "FId5-ja_8bPX",
        "outputId": "96fde5c1-7f9e-4d36-acaf-dc067746ab23"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAIBAQEBAQIBAQECAgICAgQDAgICAgUEBAMEBgUGBgYFBgYGBwkIBgcJBwYGCAsICQoKCgoKBggLDAsKDAkKCgr/wAALCABAAEABAREA/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/9oACAEBAAA/APxxtNI1vUfGy3epXyyTatc5mVjtK5PDHPSpfH+it4E8XyeH9Sj8+RGUmTzSyzZ54xxWbdXK308t3PELOVARHFbocAf3Rn+dW9OgsryyD3y4JTKggjaw6c1V0nU9VsrySC2vpIzPINxSXGMdB2roY9Nn1S0Ora7KhhhTMpkbLFs8YArG0vxNNZam/wDZN08UKfNdRx5Al9FPqKu30OkXgGuWSXMtrMTmAscqw9fbNaP7M/hWy+JXxkbwXILhZUid4SwJ3yKMDPpWT4s1vxTpHjrUNF120WR7G6eDy5k5UqSAfWqs1i18udPk3NI2SGz8rd6g0iS6lvbjRWJLDlcpjJ749Kk0DSJZtaVpbTd85yS3QD+tbnifULTS4X0qNc+b1G459ua5bTLuCCOVrm2U+ZkIh6fWrui3F9pqyrbxRtBP8zH73lgce1ejfD7Wb3w9BrHx70nFndXmv79LhiQIBCJCxHHbHFXf21/Actv4703436DZH+yvFenxXO+JWKJOR84yO+c1n/AfwTpHi7OoSXzia3kH7sW7YKng5P1r3bUv+Cdvi7xRrFvH8O7eeSa+hEpcQkLubp0PFdX8Wf8Agk38Vv2cvh1pHiHxrfWtjc3du880lw+0kk8AA9a+XfHfwmkt7c6ld65BfS7juSEA7QPeuIvNDitE8t4VXC5QqBwPriqVhbMt4YXHkqRkEvjOPX1FdL4/1kaNazeE7G3EdhZkQ2sYQ8BeCxwe5r1/9l74q+Bvib4Cb9n7483rrppm/wCJXqhj5smPTGR92v0m/wCCc/8AwTX8APpTRW+s2upWl9MvkaksAYlOucFcLX6OT/APwb+yt4UsPE3gjwDBeTpLGm27USbuev518Z/8Fp/B/wARvjV4fm8Ya/pdzZtZacGtrSGPEe0rnauK/HTSvhrrWpeENS8S23nwLpwPmRkA5PuMV43qvjfxAdQXSZrSF/JbPz/ICM8DA5Jrq/gZ8PPGP7Q3xW0v4XfDzwm0+sapdrAILfLnk4LH0A61y/jg3Opq2s7bgJMpkTzGwCO4rmPA3ijxJp2s7PsUpidwF3bmz7+1fvL/AMEev20fDfwm+Adj4f1+yOvaxezeUsCqBFp0YH3/AFJr78sf2rdL+KPha10XS/sUkskhQSPPxD+HWs39qH4l/CKz+CMv/CW6pp+uvbadItxKoEiwHbjHPevx/wDE3we0DTv2d/GnxPivIraHWryU6RAtvjcgJJIA7e9fnN4d+Hnjb4veP4/DHgTw9cX+pzXflwi3iP3s4Gcdvev6Av8Aghv/AMEgPDn7IHhmL44/FTTEu/HWswhomlg3mzVhnapIwD6mv55Bqt7c30sNvM0tsBhYZCXx9BnFTaZqKXGqw6RpmlSRyMwUDBUsx9TmvvX9j/4vfDL9nzw/YaL4wYTazNJvnjVi6hGHAyK7f/h4jovwn8Z6j4i0O+dk1KcqYoyVWJRwBjBrwH9oT9vT4p2LX9l8PtSJ0bWpPMuI5pXbaSfTH6V9z/8ABNr9nr4t/t9fsu3D/H3Rv7D8JWUPlaPcQwtHNd5HzFQx+77jrX2h+wl/wSo/ZZ/Zi1D/AISjwz4Ua71ANmO81CMA7vXk8AV9fvJPrUn9gaFK/lLxdTwEqqr/AHFIPev5T/jh/wAE0P20f2c9fuNE8Z/BTU1FtMVkvdPtXlRgO+V7V5lpHw88cnWGjk8Daot4HCqPsMoO76ba9F0T4D/tQa7ryxeHvhF4kupNqpHINOl4PqTivoH4Yf8ABEv/AIKA/tE69A0nw/utB06cr5l5qEvkqoPc5INfoj+yD/wbbfAX4MG01b9oHU5vGmsR4cactwfsyN/tc8jPY1+ib+BPBnww8Dad4d0PQbfSrCwgENlYWEQWONR6AdfqaoaRpGu+K2FnDMbWzAy5Lje6+hI6D+dejeG9B0rw/ZrBBISVGfmdWLH6V5Dc/CzXvF97JJf36S4lbDNtYFc9w3WpYv2R/A8ky358JaI80ZDedNYRFmb64zXQ2nwm8PeErdjDpenWzsMs8VoufqM960RrWhwWMFibvcYzlQRsZj+AH8603s7xdKW5s44oWYZAyru3vzn+VUrXwHeeJ50vdbneREP3Wc4I/DA/pXQRabZWMKxQBcx9NlyAD7Y5z+dQar4i0zQI2bUr9UbGTEJldiey47/hX//Z\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=64x64 at 0x7F11219ED7E0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAACnklEQVR4nO1YyZKrMAyUF/7/exOw9Q5ddAmTl4mVDJmD+kAZ8KKlLUsWCQQCgUAgEAgEAoEXkVLiEyilDF+Gzmw/7PMFLMsiu9wiknOWXdacMxt4llLQBs76Xw0rrux2HT7WWs9ugTLyXellV+DcFpFa60AtqGd99VdYJEfOpJRqrfhO8tjOg27D36vB5Sk0oKowNp611m3b+Np7772rqnydReAGXxleVFVV13WFoHiycbvd8CrfVYBr0w8pJQpKoVW1tXZuyMlvb2J6LojLV8jNX7YnNFTVnHPvHcGqtfZZBaY3U+8d50DvHRbNOVuVzv3RB42UEnfCRzDNRTC+915KWddVZgjdWvv4UTDtAXCGrEgptdbgioeA4RGOhvPuI/B4AASYpQHNLz+xbgoeD5RSGBAReZ70h3Os9EMYeBMeb1J0xpknrOBf7F0oj6dX5gOcFCKLXiQ0tQUQW2eXfggPheSYFb9iy0FcDASp3tTEM5i5DbnkmAThi+dDztmXq04rgNMU60EBnwlxJFN5m+pNwR+Sh4TCYTwm5MNUU5M4KYSsk4s5pMfYZVls/uexwuwAu7ycwsvrYAgWkVLKk7P8h3kcY1BeWV879jFjMTaV7HcFs3grlYDTfU6wdYVVZtYWnnPAep+rWlKxJxqkx8PNSul98FAIJmcqCpV4qFn1SHEUD/CetXGtldLbqPq7QE28bZutIXvv27apKr6zkhyCDC+RLFijMeH7XenFFFa25KUm+MgOGHgOkXbXXl3m4yiA1UEPVb3f71RDjuZk7jRc41mH+KKQR3Q0YGbEEF43DDekwxDISkFtRngFeQaBWKSLITEvEs9EtyLyZmm4sr7ixu581fykj707kv+QSo5ECgQCgUAgEAgEAoFX8Q8IDkExEBhF7QAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Image, display\n",
        "from tensorflow.keras.utils import load_img  #Renvoie une image sous forme d'objet PIL (Python Imaging Library), ce qui permet de la manipuler ou de l'afficher facilement\n",
        "from PIL import ImageOps\n",
        "\n",
        "# Display input image #7\n",
        "display(Image(filename=input_img_paths[7]))\n",
        "\n",
        "# Display auto-contrast version of corresponding target (per-pixel categories)\n",
        "img = ImageOps.autocontrast(load_img(target_img_paths[7]))\n",
        "display(img)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zqcw9w2NG73R"
      },
      "source": [
        "Prepare Sequence class to load & vectorize batches of data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WboNphkr8bgG"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.image import load_img,img_to_array\n",
        "# img_to_array : Convertit une image PIL en tableau NumPy\n",
        "\n",
        "# Générateur de données pour les modèles TensorFlow.\n",
        "# Elle permet de charger les images par lots au cours de l'entraînement, en minimisant la charge mémoire\n",
        "class OxfordPets(keras.utils.Sequence):\n",
        "    \"\"\"Helper to iterate over the data (as Numpy arrays).\"\"\"\n",
        "\n",
        "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths):\n",
        "        self.batch_size = batch_size\n",
        "        self.img_size = img_size\n",
        "        self.input_img_paths = input_img_paths\n",
        "        self.target_img_paths = target_img_paths\n",
        "\n",
        "    # nombre total de lots dans l'ensemble des données\n",
        "    def __len__(self):\n",
        "        return len(self.target_img_paths) // self.batch_size\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"Returns tuple (input, target) correspond to batch #idx.\"\"\"\n",
        "        i = idx * self.batch_size\n",
        "        batch_input_img_paths = self.input_img_paths[i : i + self.batch_size]\n",
        "        batch_target_img_paths = self.target_img_paths[i : i + self.batch_size]\n",
        "        x = np.zeros((self.batch_size,) + self.img_size + (3,), dtype=\"float32\")\n",
        "        for j, path in enumerate(batch_input_img_paths):\n",
        "            img = load_img(path, target_size=self.img_size)\n",
        "            x[j] = img\n",
        "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype=\"uint8\")\n",
        "        for j, path in enumerate(batch_target_img_paths):\n",
        "            img = load_img(path, target_size=self.img_size, color_mode=\"grayscale\")\n",
        "            img = img_to_array(img)\n",
        "            y[j] = np.where(img>128,1,0)\n",
        "            #y[j] = np.expand_dims(img, 2)\n",
        "            #y[j] -= 1\n",
        "        return x, y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTiaM7rFHBPn"
      },
      "source": [
        "build unet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_M8gvDua-DH4",
        "outputId": "40930d74-c79e-47a0-ce86-efc5e5ff9a7d",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_1 (InputLayer)           [(None, 64, 64, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " conv2d (Conv2D)                (None, 32, 32, 32)   896         ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " batch_normalization (BatchNorm  (None, 32, 32, 32)  128         ['conv2d[0][0]']                 \n",
            " alization)                                                                                       \n",
            "                                                                                                  \n",
            " activation (Activation)        (None, 32, 32, 32)   0           ['batch_normalization[0][0]']    \n",
            "                                                                                                  \n",
            " activation_1 (Activation)      (None, 32, 32, 32)   0           ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " separable_conv2d (SeparableCon  (None, 32, 32, 64)  2400        ['activation_1[0][0]']           \n",
            " v2D)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_1 (BatchNo  (None, 32, 32, 64)  256         ['separable_conv2d[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_2 (Activation)      (None, 32, 32, 64)   0           ['batch_normalization_1[0][0]']  \n",
            "                                                                                                  \n",
            " separable_conv2d_1 (SeparableC  (None, 32, 32, 64)  4736        ['activation_2[0][0]']           \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_2 (BatchNo  (None, 32, 32, 64)  256         ['separable_conv2d_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " max_pooling2d (MaxPooling2D)   (None, 16, 16, 64)   0           ['batch_normalization_2[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_1 (Conv2D)              (None, 16, 16, 64)   2112        ['activation[0][0]']             \n",
            "                                                                                                  \n",
            " add (Add)                      (None, 16, 16, 64)   0           ['max_pooling2d[0][0]',          \n",
            "                                                                  'conv2d_1[0][0]']               \n",
            "                                                                                                  \n",
            " activation_3 (Activation)      (None, 16, 16, 64)   0           ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " separable_conv2d_2 (SeparableC  (None, 16, 16, 128)  8896       ['activation_3[0][0]']           \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_3 (BatchNo  (None, 16, 16, 128)  512        ['separable_conv2d_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_4 (Activation)      (None, 16, 16, 128)  0           ['batch_normalization_3[0][0]']  \n",
            "                                                                                                  \n",
            " separable_conv2d_3 (SeparableC  (None, 16, 16, 128)  17664      ['activation_4[0][0]']           \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_4 (BatchNo  (None, 16, 16, 128)  512        ['separable_conv2d_3[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " max_pooling2d_1 (MaxPooling2D)  (None, 8, 8, 128)   0           ['batch_normalization_4[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_2 (Conv2D)              (None, 8, 8, 128)    8320        ['add[0][0]']                    \n",
            "                                                                                                  \n",
            " add_1 (Add)                    (None, 8, 8, 128)    0           ['max_pooling2d_1[0][0]',        \n",
            "                                                                  'conv2d_2[0][0]']               \n",
            "                                                                                                  \n",
            " activation_5 (Activation)      (None, 8, 8, 128)    0           ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " separable_conv2d_4 (SeparableC  (None, 8, 8, 256)   34176       ['activation_5[0][0]']           \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_5 (BatchNo  (None, 8, 8, 256)   1024        ['separable_conv2d_4[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_6 (Activation)      (None, 8, 8, 256)    0           ['batch_normalization_5[0][0]']  \n",
            "                                                                                                  \n",
            " separable_conv2d_5 (SeparableC  (None, 8, 8, 256)   68096       ['activation_6[0][0]']           \n",
            " onv2D)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_6 (BatchNo  (None, 8, 8, 256)   1024        ['separable_conv2d_5[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " max_pooling2d_2 (MaxPooling2D)  (None, 4, 4, 256)   0           ['batch_normalization_6[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_3 (Conv2D)              (None, 4, 4, 256)    33024       ['add_1[0][0]']                  \n",
            "                                                                                                  \n",
            " add_2 (Add)                    (None, 4, 4, 256)    0           ['max_pooling2d_2[0][0]',        \n",
            "                                                                  'conv2d_3[0][0]']               \n",
            "                                                                                                  \n",
            " activation_7 (Activation)      (None, 4, 4, 256)    0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_transpose (Conv2DTransp  (None, 4, 4, 256)   590080      ['activation_7[0][0]']           \n",
            " ose)                                                                                             \n",
            "                                                                                                  \n",
            " batch_normalization_7 (BatchNo  (None, 4, 4, 256)   1024        ['conv2d_transpose[0][0]']       \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_8 (Activation)      (None, 4, 4, 256)    0           ['batch_normalization_7[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_transpose_1 (Conv2DTran  (None, 4, 4, 256)   590080      ['activation_8[0][0]']           \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_8 (BatchNo  (None, 4, 4, 256)   1024        ['conv2d_transpose_1[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " up_sampling2d_1 (UpSampling2D)  (None, 8, 8, 256)   0           ['add_2[0][0]']                  \n",
            "                                                                                                  \n",
            " up_sampling2d (UpSampling2D)   (None, 8, 8, 256)    0           ['batch_normalization_8[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_4 (Conv2D)              (None, 8, 8, 256)    65792       ['up_sampling2d_1[0][0]']        \n",
            "                                                                                                  \n",
            " add_3 (Add)                    (None, 8, 8, 256)    0           ['up_sampling2d[0][0]',          \n",
            "                                                                  'conv2d_4[0][0]']               \n",
            "                                                                                                  \n",
            " activation_9 (Activation)      (None, 8, 8, 256)    0           ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_transpose_2 (Conv2DTran  (None, 8, 8, 128)   295040      ['activation_9[0][0]']           \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_9 (BatchNo  (None, 8, 8, 128)   512         ['conv2d_transpose_2[0][0]']     \n",
            " rmalization)                                                                                     \n",
            "                                                                                                  \n",
            " activation_10 (Activation)     (None, 8, 8, 128)    0           ['batch_normalization_9[0][0]']  \n",
            "                                                                                                  \n",
            " conv2d_transpose_3 (Conv2DTran  (None, 8, 8, 128)   147584      ['activation_10[0][0]']          \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_10 (BatchN  (None, 8, 8, 128)   512         ['conv2d_transpose_3[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " up_sampling2d_3 (UpSampling2D)  (None, 16, 16, 256)  0          ['add_3[0][0]']                  \n",
            "                                                                                                  \n",
            " up_sampling2d_2 (UpSampling2D)  (None, 16, 16, 128)  0          ['batch_normalization_10[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_5 (Conv2D)              (None, 16, 16, 128)  32896       ['up_sampling2d_3[0][0]']        \n",
            "                                                                                                  \n",
            " add_4 (Add)                    (None, 16, 16, 128)  0           ['up_sampling2d_2[0][0]',        \n",
            "                                                                  'conv2d_5[0][0]']               \n",
            "                                                                                                  \n",
            " activation_11 (Activation)     (None, 16, 16, 128)  0           ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_transpose_4 (Conv2DTran  (None, 16, 16, 64)  73792       ['activation_11[0][0]']          \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_11 (BatchN  (None, 16, 16, 64)  256         ['conv2d_transpose_4[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_12 (Activation)     (None, 16, 16, 64)   0           ['batch_normalization_11[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_transpose_5 (Conv2DTran  (None, 16, 16, 64)  36928       ['activation_12[0][0]']          \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_12 (BatchN  (None, 16, 16, 64)  256         ['conv2d_transpose_5[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " up_sampling2d_5 (UpSampling2D)  (None, 32, 32, 128)  0          ['add_4[0][0]']                  \n",
            "                                                                                                  \n",
            " up_sampling2d_4 (UpSampling2D)  (None, 32, 32, 64)  0           ['batch_normalization_12[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_6 (Conv2D)              (None, 32, 32, 64)   8256        ['up_sampling2d_5[0][0]']        \n",
            "                                                                                                  \n",
            " add_5 (Add)                    (None, 32, 32, 64)   0           ['up_sampling2d_4[0][0]',        \n",
            "                                                                  'conv2d_6[0][0]']               \n",
            "                                                                                                  \n",
            " activation_13 (Activation)     (None, 32, 32, 64)   0           ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " conv2d_transpose_6 (Conv2DTran  (None, 32, 32, 32)  18464       ['activation_13[0][0]']          \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_13 (BatchN  (None, 32, 32, 32)  128         ['conv2d_transpose_6[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " activation_14 (Activation)     (None, 32, 32, 32)   0           ['batch_normalization_13[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_transpose_7 (Conv2DTran  (None, 32, 32, 32)  9248        ['activation_14[0][0]']          \n",
            " spose)                                                                                           \n",
            "                                                                                                  \n",
            " batch_normalization_14 (BatchN  (None, 32, 32, 32)  128         ['conv2d_transpose_7[0][0]']     \n",
            " ormalization)                                                                                    \n",
            "                                                                                                  \n",
            " up_sampling2d_7 (UpSampling2D)  (None, 64, 64, 64)  0           ['add_5[0][0]']                  \n",
            "                                                                                                  \n",
            " up_sampling2d_6 (UpSampling2D)  (None, 64, 64, 32)  0           ['batch_normalization_14[0][0]'] \n",
            "                                                                                                  \n",
            " conv2d_7 (Conv2D)              (None, 64, 64, 32)   2080        ['up_sampling2d_7[0][0]']        \n",
            "                                                                                                  \n",
            " add_6 (Add)                    (None, 64, 64, 32)   0           ['up_sampling2d_6[0][0]',        \n",
            "                                                                  'conv2d_7[0][0]']               \n",
            "                                                                                                  \n",
            " conv2d_8 (Conv2D)              (None, 64, 64, 2)    578         ['add_6[0][0]']                  \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 2,058,690\n",
            "Trainable params: 2,054,914\n",
            "Non-trainable params: 3,776\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(img_size, num_classes):\n",
        "    inputs = keras.Input(shape=img_size + (3,)) #La taille d'entrée est (img_height, img_width, 3) : 3 pour RGB\n",
        "\n",
        "    ### [First half of the network: downsampling inputs (Réduction de la résolution)] ###\n",
        "\n",
        "    # Entry block\n",
        "    x = layers.Conv2D(32, 3, strides=2, padding=\"same\")(inputs)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.Activation(\"relu\")(x)\n",
        "\n",
        "    previous_block_activation = x  # Set aside residual\n",
        "\n",
        "    # Blocks 1, 2, 3 are identical apart from the feature depth.\n",
        "    for filters in [64, 128, 256]:  # filter : noyau ou kernel\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.SeparableConv2D(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.Conv2D(filters, 1, strides=2, padding=\"same\")(\n",
        "            previous_block_activation\n",
        "        )\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    ### [Second half of the network: upsampling inputs] ###\n",
        "\n",
        "    for filters in [256, 128, 64, 32]:\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.Activation(\"relu\")(x)\n",
        "        x = layers.Conv2DTranspose(filters, 3, padding=\"same\")(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "        x = layers.UpSampling2D(2)(x)\n",
        "\n",
        "        # Project residual\n",
        "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
        "        residual = layers.Conv2D(filters, 1, padding=\"same\")(residual)\n",
        "        x = layers.add([x, residual])  # Add back residual\n",
        "        previous_block_activation = x  # Set aside next residual\n",
        "\n",
        "    # Add a per-pixel classification layer\n",
        "    outputs = layers.Conv2D(num_classes, 3, activation=\"softmax\", padding=\"same\")(x)\n",
        "\n",
        "    # Define the model\n",
        "    model = keras.Model(inputs, outputs)\n",
        "    return model\n",
        "\n",
        "\n",
        "# Free up RAM in case the model definition cells were run multiple times\n",
        "keras.backend.clear_session()\n",
        "\n",
        "# Build model\n",
        "model2 = get_model(img_size, num_classes)\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o-r7bmCTHIdL"
      },
      "source": [
        "Set aside a validation split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTaUCZvtHGt1"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "# Split our img paths into a training and a validation set\n",
        "val_samples = 4000\n",
        "random.Random(2000).shuffle(input_img_paths)\n",
        "random.Random(2000).shuffle(target_img_paths)\n",
        "\n",
        "train_input_img_paths = input_img_paths[:-val_samples]\n",
        "train_target_img_paths = target_img_paths[:-val_samples]\n",
        "val_input_img_paths = input_img_paths[-val_samples:]\n",
        "val_target_img_paths = target_img_paths[-val_samples:]\n",
        "\n",
        "# Instantiate data Sequences for each split\n",
        "train_gen = OxfordPets(batch_size, img_size, train_input_img_paths, train_target_img_paths)\n",
        "val_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TNWr474zH-XO"
      },
      "source": [
        "\n",
        "\n",
        "Train the model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cicKCiwbHHMr",
        "outputId": "6bbc32eb-bc8b-4864-9ce5-9be263712a07",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "162/162 [==============================] - 415s 2s/step - loss: 0.0973 - accuracy: 0.9674 - sensitivity: 1.0000 - dice_coefficient: 0.0726 - val_loss: 0.1297 - val_accuracy: 0.9940 - val_sensitivity: 1.0000 - val_dice_coefficient: 0.0729\n",
            "Epoch 2/10\n",
            "162/162 [==============================] - 436s 3s/step - loss: 0.0547 - accuracy: 0.9671 - sensitivity: 1.0000 - dice_coefficient: 0.0726 - val_loss: 0.1223 - val_accuracy: 0.9985 - val_sensitivity: 1.0000 - val_dice_coefficient: 0.0729\n",
            "Epoch 3/10\n",
            "162/162 [==============================] - 483s 3s/step - loss: 0.0425 - accuracy: 0.9665 - sensitivity: 1.0000 - dice_coefficient: 0.0726 - val_loss: 0.0895 - val_accuracy: 0.9910 - val_sensitivity: 1.0000 - val_dice_coefficient: 0.0729\n",
            "Epoch 4/10\n",
            "162/162 [==============================] - 418s 3s/step - loss: 0.0349 - accuracy: 0.9655 - sensitivity: 1.0000 - dice_coefficient: 0.0726 - val_loss: 0.0557 - val_accuracy: 0.9754 - val_sensitivity: 1.0000 - val_dice_coefficient: 0.0729\n",
            "Epoch 5/10\n",
            "162/162 [==============================] - 423s 3s/step - loss: 0.0301 - accuracy: 0.9649 - sensitivity: 1.0000 - dice_coefficient: 0.0726 - val_loss: 0.0484 - val_accuracy: 0.9679 - val_sensitivity: 1.0000 - val_dice_coefficient: 0.0729\n",
            "Epoch 6/10\n",
            "162/162 [==============================] - 461s 3s/step - loss: 0.0263 - accuracy: 0.9643 - sensitivity: 1.0000 - dice_coefficient: 0.0726 - val_loss: 0.0493 - val_accuracy: 0.9642 - val_sensitivity: 1.0000 - val_dice_coefficient: 0.0729\n",
            "Epoch 7/10\n",
            "162/162 [==============================] - 427s 3s/step - loss: 0.0244 - accuracy: 0.9640 - sensitivity: 1.0000 - dice_coefficient: 0.0726 - val_loss: 0.0508 - val_accuracy: 0.9663 - val_sensitivity: 1.0000 - val_dice_coefficient: 0.0729\n",
            "Epoch 8/10\n",
            "162/162 [==============================] - 449s 3s/step - loss: 0.0229 - accuracy: 0.9637 - sensitivity: 1.0000 - dice_coefficient: 0.0726 - val_loss: 0.0500 - val_accuracy: 0.9615 - val_sensitivity: 1.0000 - val_dice_coefficient: 0.0729\n",
            "Epoch 9/10\n",
            "162/162 [==============================] - 375s 2s/step - loss: 0.0214 - accuracy: 0.9636 - sensitivity: 1.0000 - dice_coefficient: 0.0726 - val_loss: 0.0505 - val_accuracy: 0.9611 - val_sensitivity: 1.0000 - val_dice_coefficient: 0.0729\n",
            "Epoch 10/10\n",
            "162/162 [==============================] - 434s 3s/step - loss: 0.0196 - accuracy: 0.9634 - sensitivity: 1.0000 - dice_coefficient: 0.0726 - val_loss: 0.0512 - val_accuracy: 0.9619 - val_sensitivity: 1.0000 - val_dice_coefficient: 0.0729\n"
          ]
        }
      ],
      "source": [
        "# Configure the model for training.\n",
        "# We use the \"sparse\" version of categorical_crossentropy\n",
        "# because our target data is integers.\n",
        "#model.compile(optimizer=\"rmsprop\", loss=\"sparse_categorical_crossentropy\")\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "#sensibilité (ou recall) calcule le taux de vrais positifs\n",
        "def sensitivity(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    actual_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    return true_positives / (actual_positives + K.epsilon())\n",
        "\n",
        "#coefficient de Dice mesure la similarité entre deux ensembles(reeles et predites)\n",
        "def dice_coefficient(y_true, y_pred):\n",
        "    intersection = K.sum(y_true * y_pred)\n",
        "    union = K.sum(y_true) + K.sum(y_pred)\n",
        "    return (2 * intersection + K.epsilon()) / (union + K.epsilon())\n",
        "\n",
        "import tensorflow as tf\n",
        "metrics = [\"accuracy\", sensitivity, dice_coefficient]\n",
        "optimizer= tf.keras.optimizers.Adam(learning_rate = 1e-4)\n",
        "# loss adaptée pour des classes avec des étiquettes entières (et non des one-hot encodings)\n",
        "model2.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\", metrics=metrics)\n",
        "\n",
        "#callbacks = [\n",
        "  #  keras.callbacks.ModelCheckpoint(\"oxford_segmentation.h5\", save_best_only=True)\n",
        "#]\n",
        "\n",
        "# Train the model, doing validation at the end of each epoch.\n",
        "epochs = 10\n",
        "modelunet= model2.fit(train_gen, epochs=epochs, validation_data=val_gen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryVcvdewiy9s"
      },
      "source": [
        "Visualize predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bgosH6TRHHPI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ed437ac-9483-4f90-be21-db5748b5ab75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "125/125 [==============================] - 82s 652ms/step\n"
          ]
        }
      ],
      "source": [
        "# Generate predictions for all images in the validation set\n",
        "\n",
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import PIL\n",
        "from PIL import ImageOps\n",
        "from IPython.display import Image, display\n",
        "import shutil\n",
        "\n",
        "val_gen = OxfordPets(batch_size, img_size, val_input_img_paths, val_target_img_paths)\n",
        "val_preds = model2.predict(val_gen)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_mask(i):\n",
        "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
        "    mask = np.argmax(val_preds[i], axis=-1)\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    img = ImageOps.autocontrast(tf.keras.preprocessing.image.array_to_img(mask))\n",
        "    display(img)\n",
        "\n",
        "\n",
        "# Display results for validation image\n",
        "i = 4\n",
        "\n",
        "# Display input image\n",
        "display(Image(filename=val_input_img_paths[i]))\n",
        "\n",
        "# Display ground-truth target mask\n",
        "img = ImageOps.autocontrast(load_img(val_target_img_paths[i]))\n",
        "display(img)\n",
        "\n",
        "# Display mask predicted by our model\n",
        "display_mask(i)  # Note that the model only sees inputs at 150x150.\n",
        "\n",
        "\n",
        "PATH_DATABASE ='/content/drive/MyDrive/evaluation/'\n",
        "\n",
        "PATH_NODULE_UNET_IMAGE = PATH_DATABASE + 'evaluation_nodules/'\n",
        "PATH_NODULE_MASK_UNET_IMAGE = PATH_DATABASE + 'evaluation_mask/'\n",
        "PATH_NODULE_MASK_GENERATED_UNET_IMAGE = PATH_DATABASE + 'mask_unet/'\n",
        "\n",
        "dir_unet_nodule = PATH_NODULE_UNET_IMAGE\n",
        "dir_unet_mask = PATH_NODULE_MASK_UNET_IMAGE\n",
        "dir_unet_generated_mask = PATH_NODULE_MASK_GENERATED_UNET_IMAGE\n",
        "\n",
        "for i in range(len(val_preds)):\n",
        "\n",
        "    filelocation = val_input_img_paths[i]\n",
        "    maskLocation = val_target_img_paths[i]\n",
        "\n",
        "    #Extrait le nom du fichier à partir du chemin complet.\n",
        "    filename = filelocation.split(os.path.sep)[-1]\n",
        "\n",
        "    label = filename[-5:-4]\n",
        "    if label in '12' :\n",
        "        label = '1'\n",
        "    elif label in '45' :\n",
        "        label = '2'\n",
        "    else :\n",
        "        label = '0'\n",
        "\n",
        "    imgnod = dir_unet_nodule + '\\\\' +  label + filename\n",
        "    shutil.copy2(filelocation,imgnod)\n",
        "\n",
        "    imgnodmask = dir_unet_mask + '\\\\' +  label + filename\n",
        "    shutil.copy2(maskLocation,imgnodmask)\n",
        "\n",
        "    imgunetmask = dir_unet_generated_mask+ '\\\\' + label + filename\n",
        "    mask = np.argmax(val_preds[i], axis=-1)\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    img = PIL.ImageOps.autocontrast(tf.keras.preprocessing.image.array_to_img(mask))\n",
        "    img.save(imgunetmask)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "id": "4v3egWlaaKZA",
        "outputId": "44dea5f3-61d7-43ab-dc95-2e31a501a31f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=64x64 at 0x7F1123637BB0>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAIAAAAlC+aJAAACcElEQVR4nO1Y0W7DIBA7uOT/f7dtAuzBimWRblJoNjHp/FBBII0PjO8Ss0AgEAgEAoFAIBC4HTlnM0spaXdqKMVzm/Esy/L33C6AS37u5pzR7eZMgZwz1tjdeRFEl2XpGE8qJ7JMKb2leI5wIpBWSkkPbmuttVZr1WmTxoAFJuN939EupfDi7eq/WYugi4a7t9bMLOeMGFJK3Iq7cDkA6lhtHr/7vmOo1sozgBgYjJl9F8PY5ny0oe5eSkkpUeUkgSvQeimFBmpHSHAtiAqbwwgvYURC7k5meLC78+xiH/Qovx1iGBSbBvmL0Gcgp+act22rtdZaofVaazsBF0spmNlaez6fZrau60d8Bu7B1jOeTtN6ADCk3XN+YLLb9x2avEbmKnsyhoqwuiCHNs1U71LB0GQ5tK4rurd71HvopqvT0++RAV6vF7povE0L6H5SIF2+h/pBA1Tw7FIKU6waC30GQ5zGizzWA150ZyLTAoFSBkXErDmB7DXlDeByALVWSOjtOSYPLfpBkVLZto27gTyARLEsy0AYlyXEXaZjMBnpHBoOhdEJycSjcLjHctn9NmqHXbJ0o4pQFKlyTIoRONhVIxq3UTPDprs7TBBWo5xUKnyxBHiO0cDogI2O7ADrCDtWzt1V2XakBXUbPbVd4YCFH1j+T9FpQLMBPV6N//F4sLJIB/BXaPzFq+YP5bTmZmYu5go2+Fe3sL+5+oNUvjMTrUOnQ6eHbjlV8TpzOnTU+a2F3Xmpk6h+nugaHJr0uxBBd1KrmfRTigIvlj/PSSn9g0hYyekZmP2bLomepX9uzH4GAoFAIBAIBAKBwL/AF4tOYaM5xPmOAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<PIL.Image.Image image mode=L size=64x64 at 0x7F1123637C10>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAEAAAABACAAAAACPAi4CAAAAcklEQVR4nO2TwQrAMAhD7f7/n9PDYIzWuAhj7JB3E2NqShthjDHmNUbZRQw8iKoeFFXRwr2gOtrAUjPhIc5TmIHsSAzkBUi0fD7VyhEYnxqkwToGP70D9kKTvLlU34AcxQw2efczrQP973wCRWSMMcZcTAAODCI58rK6AAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualize figure Loss & Accuracy"
      ],
      "metadata": {
        "id": "vJ_QL2dAc92r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib\n",
        "matplotlib.use(\"Agg\")\n",
        "\n",
        "\n",
        "loss = modelunet.history['loss']\n",
        "val_loss =modelunet.history['val_loss']\n",
        "\n",
        "plt.figure()\n",
        "plt.plot( loss, 'r', label='Training loss')\n",
        "plt.plot( val_loss, 'bo', label='Validation loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss Value')\n",
        "plt.ylim([0, 1])\n",
        "plt.legend()\n",
        "plt.savefig('/content/drive/MyDrive/evaluation/fig.png')\n",
        "plt.show()\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(modelunet.history['accuracy'])\n",
        "plt.plot(modelunet.history['val_accuracy'])\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.grid(True)\n",
        "plt.savefig('/content/drive/MyDrive/evaluation/figAcc.png')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "15mFTkgseP4Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "787qZVSSLBAk"
      },
      "source": [
        "Performence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BS3Nx3lZLAmi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import keras as k\n",
        "import numpy as np\n",
        "import keras as k\n",
        "\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def dice_coefficient(y_true, y_pred):\n",
        "    intersection = K.sum(y_true * y_pred)\n",
        "    union = K.sum(y_true) + K.sum(y_pred)\n",
        "    return (2 * intersection + K.epsilon()) / (union + K.epsilon())\n",
        "\n",
        "def sensitivity(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    actual_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    return true_positives / (actual_positives + K.epsilon())\n",
        "\n",
        "def iou_coef(save_images,test_y):\n",
        "    intersection = K.sum(save_images*test_y)\n",
        "    sum = K.sum(save_images+test_y)\n",
        "    iou = (intersection ) / (sum - intersection )\n",
        "    return iou"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PLu5Ik7dLjMV"
      },
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "val_input_img_paths='/content/drive/MyDrive/testing/nodules'\n",
        "val_target_img_paths='/content/drive/MyDrive/testing/mask'\n",
        "\n",
        "input_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(val_input_img_paths, fname)\n",
        "        for fname in os.listdir(val_input_img_paths)\n",
        "        if fname.endswith(\".jpg\")\n",
        "    ]\n",
        ")\n",
        "target_img_paths = sorted(\n",
        "    [\n",
        "        os.path.join(val_target_img_paths, fname)\n",
        "        for fname in os.listdir(val_target_img_paths)\n",
        "        if fname.endswith(\".jpg\") and not fname.startswith(\".\")\n",
        "    ]\n",
        ")\n",
        "\n",
        "val_gen = OxfordPets(batch_size, img_size, input_img_paths, target_img_paths)\n",
        "val_preds = model2.predict(val_gen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phk9qycfmS0G",
        "outputId": "36a3c14f-ee3d-4e89-a250-4bf1eb2816e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "128/128 [==============================] - 116s 904ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def display_mask(i):\n",
        "    \"\"\"Quick utility to display a model's prediction.\"\"\"\n",
        "    mask = np.argmax(val_preds[i], axis=-1)\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    img = ImageOps.autocontrast(tf.keras.preprocessing.image.array_to_img(mask))\n",
        "    display(img)\n",
        "\n",
        "\n",
        "# Display results for validation image\n",
        "i = 4\n",
        "\n",
        "# Display input image\n",
        "display(Image(filename=val_input_img_paths[i]))\n",
        "\n",
        "# Display ground-truth target mask\n",
        "img = ImageOps.autocontrast(load_img(val_target_img_paths[i]))\n",
        "display(img)\n",
        "\n",
        "# Display mask predicted by our model\n",
        "display_mask(i)  # Note that the model only sees inputs at 150x150.\n",
        "\n",
        "PATH_DATABASE ='/content/drive/MyDrive/testing/'\n",
        "\n",
        "PATH_NODULE_UNET_IMAGE = PATH_DATABASE + 'nodules/'\n",
        "PATH_NODULE_MASK_UNET_IMAGE = PATH_DATABASE + 'mask/'\n",
        "PATH_NODULE_MASK_GENERATED_UNET_IMAGE = PATH_DATABASE + 'PredectionForEvaluation/'\n",
        "\n",
        "dir_unet_nodule = PATH_NODULE_UNET_IMAGE\n",
        "dir_unet_mask = PATH_NODULE_MASK_UNET_IMAGE\n",
        "dir_unet_generated_mask = PATH_NODULE_MASK_GENERATED_UNET_IMAGE\n",
        "\n",
        "\n",
        "\n",
        "for i in range(len(val_preds)):\n",
        "\n",
        "    filelocation = val_input_img_paths[i]\n",
        "    maskLocation = val_target_img_paths[i]\n",
        "\n",
        "    filename = filelocation.split(os.path.sep)[-1]\n",
        "\n",
        "    label = filename[-5:-4]\n",
        "    if label in '12' :\n",
        "        label = '1'\n",
        "    elif label in '45' :\n",
        "        label = '2'\n",
        "    else :\n",
        "        label = '0'\n",
        "\n",
        "    imgnod = dir_unet_nodule + '\\\\' +  label + filename\n",
        "    shutil.copy2(filelocation,imgnod)\n",
        "\n",
        "    imgnodmask = dir_unet_mask + '\\\\' +  label + filename\n",
        "    shutil.copy2(maskLocation,imgnodmask)\n",
        "\n",
        "    imgunetmask = dir_unet_generated_mask+ '\\\\' + label + filename\n",
        "    mask = np.argmax(val_preds[i], axis=-1)\n",
        "    mask = np.expand_dims(mask, axis=-1)\n",
        "    img = PIL.ImageOps.autocontrast(tf.keras.preprocessing.image.array_to_img(mask))\n",
        "    img.save(imgunetmask)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "y8IbrLAGwDoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "from os import listdir\n",
        "import keras as k\n",
        "evaluation_mask='/content/drive/MyDrive/evaluation/evaluation_mask'\n",
        "mask_unet='/content/drive/MyDrive/evaluation/mask_unet'\n",
        "dices  = []\n",
        "sensitivities=[]\n",
        "iuo =[]\n",
        "\n",
        "for image_file in os.listdir(evaluation_mask):\n",
        "    for pred_file in os.listdir(mask_unet):\n",
        "        image_path = os.path.join(evaluation_mask, image_file)\n",
        "        pred_path = os.path.join(mask_unet, pred_file)\n",
        "\n",
        "        dices.append(dice_coefficient(pred_path, image_path))\n",
        "        sensitivities.append(sensitivity(pred_path, image_path))\n",
        "        iuo.append(iou_coef(pred_path, image_path))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Mu4uLmy0j8Mn",
        "outputId": "0e7ed91f-f32a-424a-b64f-a0ce2ec33e51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-aa6dd3e716a9>\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mpred_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmask_unet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdice_coefficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0msensitivities\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msensitivity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0miuo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miou_coef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-48-9615c8e264fc>\u001b[0m in \u001b[0;36mdice_coefficient\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdice_coefficient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mintersection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0munion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mintersection\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0munion\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'str'"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}